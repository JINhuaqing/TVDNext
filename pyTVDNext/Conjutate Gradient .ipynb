{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f192469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from easydict import EasyDict as edict\n",
    "from collections import defaultdict as ddict\n",
    "import torch\n",
    "import time\n",
    "from tqdm.autonotebook import tqdm\n",
    "from scipy import signal\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe57a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccd7a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepOpt():\n",
    "    \"\"\"\n",
    "        I concatenate the real and image part into one vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, Y, pUinv, fixedParas, lastTheta, **paras):\n",
    "        \"\"\"\n",
    "         Input: \n",
    "             Y: A tensor with shape, d x dF x dT\n",
    "             X: A tensor with shape, d x dF x dT\n",
    "             pUinv, the first R row of the inverse of the eigen vector matrix, R x d, complex data\n",
    "             fixedParas: The fixed parameters when optimizing, real data \n",
    "                 when update \\mu, 2R x dT\n",
    "                 when update \\nu, 2R x dF\n",
    "             lastTheta: The parameters for optimizing at the last time step, initial parameters, vector of 2R(D-1), real data\n",
    "             paras:\n",
    "                 beta: tuning parameter for iteration\n",
    "                 alp: tuning parameter for iteration\n",
    "                 rho: a vector of length (D-1)2R, real data\n",
    "                 lam: the parameter for SCAD\n",
    "                 a: the parameter for SCAD, > 1+1/beta\n",
    "                 iterNum:  integer, number of iterations\n",
    "                 iterC: decimal, stopping rule\n",
    "                 eps: decimal, stopping rule for conjugate gradient method\n",
    "        \"\"\"\n",
    "        \n",
    "        parasDefVs = {\"a\": 2.7,  \"beta\": 1, \"alp\": 0.9,  \"rho\": None,  \"lam\": 1e2, \n",
    "                      \"iterNum\": 100, \"iterC\": 1e-4, \"eps\": 1e-6}\n",
    "        \n",
    "        self.paras = edict(parasDefVs)\n",
    "        for key in paras.keys():\n",
    "            self.paras[key] = paras[key]\n",
    "        \n",
    "            \n",
    "            \n",
    "        self.d, self.dF, self.dT = X.shape\n",
    "        self.R2, _ = fixedParas.shape\n",
    "        self.D = int(lastTheta.shape[0]/self.R2+1)\n",
    "        self.nD = self.dF if self.D == self.dT else self.dT\n",
    "        \n",
    "        self.pUinv = pUinv\n",
    "        self.X = X.type_as(self.pUinv)\n",
    "        self.Y = Y.type_as(self.pUinv) # Make them complex\n",
    "        \n",
    "        R = int(self.R2/2)\n",
    "        if self.paras.rho is None:\n",
    "            self.paras.rho = torch.ones(self.R2*(self.D-1))\n",
    "            \n",
    "            \n",
    "        self.fixedParas = torch.complex(fixedParas[:R, :], fixedParas[R:, :]).type_as(self.pUinv) # R x nD\n",
    "        \n",
    "        self.lastTheta= lastTheta\n",
    "        \n",
    "        \n",
    "        self.newVecGam = None\n",
    "        self.newVecGamStd = None\n",
    "        self.halfRho = None\n",
    "        self.rho = self.paras.rho\n",
    "        self.lam = self.paras.lam\n",
    "        self.a = self.paras.a \n",
    "        self.iterNum = self.paras.iterNum\n",
    "        self.iterC = self.paras.iterC\n",
    "            \n",
    "        self.leftMat = None\n",
    "        self.leftMatVec = None\n",
    "        self.NewXYR2Sum = None\n",
    "        \n",
    "    def obtainNewData(self):\n",
    "        pY = self.Y.permute(1, 2, 0) # dF x dT x d\n",
    "        pX = self.X.permute(1, 2, 0)\n",
    "        cNewX = pX.matmul(self.pUinv.T)  # dF x dT x R\n",
    "        if self.D == self.dF:\n",
    "            cNewY = pY.matmul(self.pUinv.T) * (1/self.fixedParas.T) # dF x dT x R\n",
    "        else:\n",
    "            cNewY = pY.matmul(self.pUinv.T) * (1/self.fixedParas.T).unsqueeze(1) # dF x dT x R\n",
    "        self.NewXr = cNewX.real\n",
    "        self.NewYr = cNewY.real\n",
    "        self.NewXi = cNewX.imag\n",
    "        self.NewYi = cNewY.imag\n",
    "        \n",
    "    def _AmatOpt(self, vec):\n",
    "        rVec1 = self.leftMatVecP1 * vec\n",
    "        rVec2 = self.paras.beta * DiffMatTOpt(DiffMatOpt(vec, self.R2), self.R2)\n",
    "        return rVec1 + rVec2\n",
    "    \n",
    "    def _ConjuGrad(self, vec, maxIter=1000):\n",
    "        \"\"\" \n",
    "        Ax = vec\n",
    "        \"\"\"\n",
    "        eps = self.paras.eps\n",
    "        \n",
    "        xk = torch.zeros_like(vec)\n",
    "        rk = vec - self._AmatOpt(xk)\n",
    "        pk = rk\n",
    "        if torch.norm(rk) <= eps:\n",
    "            return xk\n",
    "        \n",
    "        for k in range(maxIter):\n",
    "            alpk = torch.sum(rk**2) / torch.sum(pk * self._AmatOpt(pk))\n",
    "            xk = xk + alpk * pk \n",
    "            \n",
    "            rk_1 = rk\n",
    "            rk = rk - alpk * self._AmatOpt(pk)\n",
    "            \n",
    "            if torch.norm(rk) <= eps:\n",
    "                break \n",
    "                \n",
    "            betk = torch.sum(rk**2)/torch.sum(rk_1**2)\n",
    "            pk = rk + betk * pk\n",
    "            \n",
    "        return xk\n",
    "        \n",
    "    \n",
    "    def updateVecGam(self):\n",
    "        \"\"\"\n",
    "            I use conjugate gradient to solve it. \n",
    "            Update the Gamma matrix, first step \n",
    "        \"\"\"\n",
    "        self.DiffMatSq = genDiffMatSqfn(self.R2, self.D) # R2D x R2D\n",
    "        \n",
    "        optAxis = 1 if self.D == self.dF else 0\n",
    "        \n",
    "        if self.leftMat is None:\n",
    "            NewXSq = self.NewXr**2 + self.NewXi**2\n",
    "            NewXSqR2 = torch.cat((NewXSq, NewXSq), dim=2) # dF x dT x 2R\n",
    "            NewXSqR2Sum = NewXSqR2.sum(axis=optAxis) # dF x 2R or dT x 2R\n",
    "            self.leftMat = torch.diag(NewXSqR2Sum.flatten()).to_sparse()/self.nD +  \\\n",
    "                    self.paras.beta * self.DiffMatSq\n",
    "        \n",
    "        if self.NewXYR2Sum is None:\n",
    "            NewXY1 = self.NewXr * self.NewYr + self.NewXi * self.NewYi\n",
    "            NewXY2 = -self.NewXi * self.NewYr + self.NewXr * self.NewYi\n",
    "            NewXYR2 = torch.cat((NewXY1, NewXY2), dim=2) # dF x dT x 2R\n",
    "            self.NewXYR2Sum = NewXYR2.sum(axis=optAxis) # dF x 2R or dT x 2R\n",
    "        rightVec = self.NewXYR2Sum.flatten()/self.nD + \\\n",
    "                    DiffMatTOpt(self.rho + self.paras.beta * self.lastTheta, self.R2)\n",
    "        \n",
    "        # self.newVecGam, = torch.inverse(self.leftMat).matmul(rightVec)\n",
    "        # Better way to do so\n",
    "        self.newVecGam, _  = torch.solve(rightVec.reshape(-1, 1), self.leftMat.to_dense()) \n",
    "        self.newVecGam = self.newVecGam.reshape(-1)\n",
    "        \n",
    "    def updateVecGamConGra(self):\n",
    "        \"\"\"\n",
    "            Update the Gamma matrix, first step, wth Conjugate Gradient Method\n",
    "        \"\"\"\n",
    "        optAxis = 1 if self.D == self.dF else 0\n",
    "        \n",
    "        if self.leftMat is None:\n",
    "            NewXSq = self.NewXr**2 + self.NewXi**2\n",
    "            NewXSqR2 = torch.cat((NewXSq, NewXSq), dim=2) # dF x dT x 2R\n",
    "            NewXSqR2Sum = NewXSqR2.sum(axis=optAxis) # dF x 2R or dT x 2R\n",
    "            self.leftMatVecP1 = NewXSqR2Sum.flatten()/self.nD\n",
    "        \n",
    "        if self.NewXYR2Sum is None:\n",
    "            NewXY1 = self.NewXr * self.NewYr + self.NewXi * self.NewYi\n",
    "            NewXY2 = -self.NewXi * self.NewYr + self.NewXr * self.NewYi\n",
    "            NewXYR2 = torch.cat((NewXY1, NewXY2), dim=2) # dF x dT x 2R\n",
    "            self.NewXYR2Sum = NewXYR2.sum(axis=optAxis) # dF x 2R or dT x 2R\n",
    "        rightVec = self.NewXYR2Sum.flatten()/self.nD + \\\n",
    "                    DiffMatTOpt(self.rho + self.paras.beta * self.lastTheta, self.R2)\n",
    "        \n",
    "        # self.newVecGam, = torch.inverse(self.leftMat).matmul(rightVec)\n",
    "        # Better way to do so\n",
    "        self.newVecGam = self._ConjuGrad(rightVec)\n",
    "        \n",
    "    def updateHRho(self):\n",
    "        \"\"\"\n",
    "            Update the vector rho at 1/2 step, second step\n",
    "        \"\"\"\n",
    "        halfRho = self.rho - self.paras.alp * self.paras.beta * (DiffMatOpt(self.newVecGam, self.R2) - self.lastTheta)\n",
    "        self.halfRho = halfRho\n",
    "       \n",
    "    \n",
    "    def updateTheta(self):\n",
    "        \"\"\"\n",
    "            Update the vector Theta, third step\n",
    "        \"\"\"\n",
    "        halfTheta = DiffMatOpt(self.newVecGam, self.R2) - self.halfRho/self.paras.beta\n",
    "        tranHTheta = halfTheta.reshape(-1, self.R2) # D-1 x 2R\n",
    "        hThetaL2Norm = tranHTheta.abs().square().sum(axis=1).sqrt() # D-1\n",
    "        normCs = torch.zeros_like(hThetaL2Norm) - 1\n",
    "        \n",
    "        normC1 = hThetaL2Norm - self.lam/self.paras.beta\n",
    "        normC1[normC1<0] = 0\n",
    "        \n",
    "        normC2 = (self.paras.beta * (self.a - 1) * hThetaL2Norm - self.a * self.lam)/(self.paras.beta * self.a - self.paras.beta -1)\n",
    "        \n",
    "        c1 = (1+1/self.paras.beta)* self.lam\n",
    "        c2 = self.a * self.lam\n",
    "        \n",
    "        normCs[hThetaL2Norm<=c1] = normC1[hThetaL2Norm<=c1]\n",
    "        normCs[hThetaL2Norm>c2] = hThetaL2Norm[hThetaL2Norm>c2]\n",
    "        normCs[normCs==-1] = normC2[normCs==-1]\n",
    "        \n",
    "        normCs[normCs!=0] = normCs[normCs!=0]/hThetaL2Norm[normCs!=0]\n",
    "        \n",
    "        self.lastTheta = (tranHTheta*normCs.reshape(-1, 1)).flatten()\n",
    "        \n",
    "    \n",
    "    def updateRho(self):\n",
    "        \"\"\"\n",
    "            Update the vector rho, fourth step\n",
    "        \"\"\"\n",
    "        newRho = self.halfRho - self.paras.alp * self.paras.beta * (DiffMatOpt(self.newVecGam, self.R2) - self.lastTheta)\n",
    "        self.rho = newRho\n",
    "        \n",
    "    \n",
    "    def __call__(self, is_showProg=False, leave=False):\n",
    "        self.obtainNewData()\n",
    "        if self.iterC is None:\n",
    "            self.iterC = 0\n",
    "        \n",
    "        chDiff = torch.tensor(1e10)\n",
    "        self.updateVecGamConGra()\n",
    "        self.updateHRho()\n",
    "        self.updateTheta()\n",
    "        self.updateRho()\n",
    "        lastVecGam = self.newVecGam\n",
    "        if is_showProg:\n",
    "            with tqdm(total=self.iterNum, leave=leave) as pbar:\n",
    "                for i in range(self.iterNum):\n",
    "                    pbar.set_description(f\"Inner Loop: The chdiff is {chDiff.item():.3e}.\")\n",
    "                    pbar.update(1)\n",
    "                    self.updateVecGamConGra()\n",
    "                    self.updateHRho()\n",
    "                    self.updateTheta()\n",
    "                    self.updateRho()\n",
    "                    chDiff = torch.norm(self.newVecGam-lastVecGam)/torch.norm(lastVecGam)\n",
    "                    lastVecGam = self.newVecGam\n",
    "                    if chDiff < self.iterC:\n",
    "                        pbar.update(self.iterNum)\n",
    "                        break\n",
    "        else:\n",
    "            for i in range(self.iterNum):\n",
    "                self.updateVecGamConGra()\n",
    "                self.updateHRho()\n",
    "                self.updateTheta()\n",
    "                self.updateRho()\n",
    "                chDiff = torch.norm(self.newVecGam-lastVecGam)/torch.norm(lastVecGam)\n",
    "                lastVecGam = self.newVecGam\n",
    "                if chDiff < self.iterC:\n",
    "                    break\n",
    "            \n",
    "        if self.D == self.dF:\n",
    "            R = int(self.R2/2)\n",
    "            newGam = self.newVecGam.reshape(-1, self.R2) # D x 2R\n",
    "            newGamNorm2 = newGam.square().sum(axis=0) # 2R\n",
    "            newGamNorm = torch.sqrt(newGamNorm2[:R] + newGamNorm2[R:])\n",
    "            newGamNorm = torch.cat([newGamNorm, newGamNorm])\n",
    "            newGam = newGam/newGamNorm\n",
    "            self.newVecGamStd = newGam.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e5c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a354e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = Path(\"../data\")\n",
    "datF = list(dataPath.glob(\"*.mat\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068cde0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDat = loadmat(datF)\n",
    "dat = rawDat[\"DK_timecourse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "187f1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 600\n",
    "Rn = 5\n",
    "outIterC = 1e-5\n",
    "lams = [5e2, 1e2]\n",
    "q = 1\n",
    "downrates = [1, 1]\n",
    "hs = [0.1, 0.1]\n",
    "T = 6\n",
    "iterNums = [100, 100]\n",
    "betas = [10, 10]\n",
    "iterCs = [1e-4, 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22074d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fOpt = TVDNextOpt(rawDat=dat, fs=fs, T=T, hs=hs, Rn=Rn, lams=lams, downrates=downrates, q=q, \n",
    "                  iterNums=iterNums, iterCs=iterCs, \n",
    "                  outIterC=outIterC, maxIter=1000, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25ca91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fOpt._PreProcess()\n",
    "fOpt._estAmat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f26ef6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, fOpt.dF, fOpt.dT = fOpt.X.shape\n",
    "fOpt.paras.paraMuInit = torch.rand(fOpt.R2, fOpt.dF)\n",
    "fOpt.paras.paraNuInit = torch.rand(fOpt.R2, fOpt.dT)\n",
    "lastNuTheta = DiffMatOpt(colStackFn(fOpt.paras.paraNuInit), fOpt.R2)\n",
    "fixedMuMat = fOpt.paras.paraMuInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "509c29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = torch.ones(fOpt.R2*(fOpt.dT-1))\n",
    "lam = 1\n",
    "iterNum = 1000\n",
    "optNu1 = OneStepOpt(X=fOpt.X, Y=fOpt.Y, pUinv=fOpt.pUinv, fixedParas=fixedMuMat, lastTheta=lastNuTheta, \n",
    "                               alp=0.9, beta=1, lam=lam, \n",
    "                               a=2.7, iterNum=iterNum, rho=rho, iterC=1e-5, eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2329c34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e7280cd03d466395f7d750e6859b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optNu1(True, leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f011dd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7399e-02, -1.9078e-01, -4.9737e-01,  ...,  1.0160e+00,\n",
       "          1.3673e+00,  1.4837e+00],\n",
       "        [-1.7804e-02, -2.2225e-01, -7.2426e-01,  ...,  6.9845e-01,\n",
       "          6.7883e-01,  7.0824e-01],\n",
       "        [-4.1351e-05,  7.8151e-06,  1.5950e-06,  ...,  1.4594e+00,\n",
       "          1.3090e+00,  1.1016e+00],\n",
       "        ...,\n",
       "        [ 5.3989e-03,  6.7355e-02,  2.1943e-01,  ..., -9.1689e-01,\n",
       "         -9.5823e-01, -9.9974e-01],\n",
       "        [ 8.8812e-05, -1.6764e-05, -3.4021e-06,  ..., -3.7566e+00,\n",
       "         -3.3763e+00, -2.8216e+00],\n",
       "        [ 2.6780e-01,  2.8097e+00,  6.2698e+00,  ..., -8.2912e-01,\n",
       "         -8.2856e-01, -6.5083e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colStackFn(optNu1.newVecGam, optNu1.R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a83bb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6340e-01, -2.0449e-01,  5.2005e-05,  ..., -4.1522e-02,\n",
       "         5.5475e-01,  1.7776e-01])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optNu1.lastTheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ec37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
